{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpcTL4f2G9WE"
      },
      "source": [
        "## libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PM2h7czt2s0f"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet transformers==4.5.0\n",
        "!pip install --quiet sentencepiece==0.1.95\n",
        "!pip install --quiet textwrap3==0.9.2\n",
        "!pip install --quiet nltk==3.2.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cDHhiuKJ2eva",
        "outputId": "b6ff4dce-1de5-44d4-bfd1-40b3dfad07ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25htime: 375 µs (started: 2024-04-30 08:03:53 +00:00)\n"
          ]
        }
      ],
      "source": [
        "!pip install --quiet ipython-autotime\n",
        "%load_ext autotime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScrtI0ueTFbR"
      },
      "source": [
        "## Text wrapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VrMhx6BAP2Ez",
        "outputId": "c6c3f4c3-aa7b-4617-de5d-0f83f6ea7141"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Orcas, also known as killer whales, are one of the most awe-inspiring and iconic aquatic animals found in the world's oceans. Belonging to the family\n",
            "of dolphins, they are characterized by their distinctive black and white markings, impressive size, and powerful presence. Orcas are highly social\n",
            "creatures, living in tight-knit pods led by a matriarchal hierarchy. These pods often consist of multiple generations, with individuals forming strong\n",
            "bonds through complex communication and cooperative hunting strategies.  Remarkably intelligent and adaptable, orcas have earned a reputation as apex\n",
            "predators, capable of hunting a wide variety of prey, including fish, seals, and even other marine mammals like dolphins and whales. They employ\n",
            "sophisticated hunting techniques, such as coordinated teamwork and strategic maneuvers, to outsmart their prey. Despite their formidable hunting\n",
            "prowess, orcas are also known for their playful behavior, often engaging in acrobatic displays and social interactions, including breaching, tail-\n",
            "slapping, and vocalizations.  Orcas occupy a diverse range of habitats, from polar regions to tropical seas, demonstrating their remarkable ability to\n",
            "thrive in various environments. They have been observed in all the world's oceans, from the Arctic to the Antarctic, showcasing their adaptability to\n",
            "different climates and ecosystems. However, like many marine species, orcas face numerous threats, including habitat degradation, pollution, climate\n",
            "change, and overfishing of their prey species.  Conservation efforts are crucial to safeguard the future of orcas and their marine habitats. By\n",
            "protecting their habitats, regulating human activities, and promoting sustainable fishing practices, we can help ensure the survival of these\n",
            "magnificent creatures. Orcas serve as ambassadors for the health of our oceans, highlighting the interconnectedness of all marine life and the\n",
            "importance of preserving these ecosystems for future generations to enjoy and appreciate. Through research, education, and collaborative conservation\n",
            "initiatives, we can work together to ensure a brighter future for orcas and all aquatic animals.\n",
            "\n",
            "\n",
            "time: 1.98 ms (started: 2024-04-30 08:03:53 +00:00)\n"
          ]
        }
      ],
      "source": [
        "text = \"\"\"Orcas, also known as killer whales, are one of the most awe-inspiring and iconic aquatic animals found in the world's oceans. Belonging to the family of dolphins, they are characterized by their distinctive black and white markings, impressive size, and powerful presence. Orcas are highly social creatures, living in tight-knit pods led by a matriarchal hierarchy. These pods often consist of multiple generations, with individuals forming strong bonds through complex communication and cooperative hunting strategies.\n",
        "\n",
        "Remarkably intelligent and adaptable, orcas have earned a reputation as apex predators, capable of hunting a wide variety of prey, including fish, seals, and even other marine mammals like dolphins and whales. They employ sophisticated hunting techniques, such as coordinated teamwork and strategic maneuvers, to outsmart their prey. Despite their formidable hunting prowess, orcas are also known for their playful behavior, often engaging in acrobatic displays and social interactions, including breaching, tail-slapping, and vocalizations.\n",
        "\n",
        "Orcas occupy a diverse range of habitats, from polar regions to tropical seas, demonstrating their remarkable ability to thrive in various environments. They have been observed in all the world's oceans, from the Arctic to the Antarctic, showcasing their adaptability to different climates and ecosystems. However, like many marine species, orcas face numerous threats, including habitat degradation, pollution, climate change, and overfishing of their prey species.\n",
        "\n",
        "Conservation efforts are crucial to safeguard the future of orcas and their marine habitats. By protecting their habitats, regulating human activities, and promoting sustainable fishing practices, we can help ensure the survival of these magnificent creatures. Orcas serve as ambassadors for the health of our oceans, highlighting the interconnectedness of all marine life and the importance of preserving these ecosystems for future generations to enjoy and appreciate. Through research, education, and collaborative conservation initiatives, we can work together to ensure a brighter future for orcas and all aquatic animals. \"\"\"\n",
        "for wrp in wrap(text, 150):\n",
        "  print (wrp)\n",
        "print (\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imbR470g15Fq"
      },
      "source": [
        "# **Summarization with T5 Transformer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cXs7fnvCarm"
      },
      "outputs": [],
      "source": [
        "# Import the necessary libraries for torch and transformers\n",
        "import torch\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "# Load the pre-trained T5 model and tokenizer for text summarization\n",
        "summary_model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
        "summary_tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
        "\n",
        "# Set the device to use for computations (GPU or CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move the model to the selected device\n",
        "summary_model = summary_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzGsTUJ8TyAN"
      },
      "outputs": [],
      "source": [
        "# Import the necessary libraries for random number generation and NLTK\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Define a function to set the random seed for reproducibility\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# Set the random seed for reproducibility\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8JaEy5Xw_UMf"
      },
      "outputs": [],
      "source": [
        "# Import the necessary NLTK libraries for text processing\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('brown')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Import the necessary NLTK modules for word sense disambiguation and text tokenization\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Define a function to post-process the text by capitalizing each sentence\n",
        "def postprocesstext (content):\n",
        "  final=\"\"\n",
        "  for sent in sent_tokenize(content):\n",
        "    sent = sent.capitalize()\n",
        "    final = final +\" \"+sent\n",
        "  return final\n",
        "\n",
        "# Define a function to generate the summary of the text using the pre-trained T5 model\n",
        "def summarizer(text, model, tokenizer):\n",
        "\n",
        "  # Pre-process the text by adding a \"summarize\" prefix and removing newline characters\n",
        "  text = text.strip().replace(\"\\n\", \" \")\n",
        "  text = \"summarize: \" + text\n",
        "\n",
        "  # Encode the text using the tokenizer and set the max length to 512\n",
        "  max_len = 512\n",
        "  encoding = tokenizer.encode_plus(text, max_length=max_len, pad_to_max_length=False, truncation=True, return_tensors=\"pt\").to(device)\n",
        "\n",
        "  # Extract the input_ids and attention_mask from the encoding\n",
        "  input_ids, attention_mask = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
        "\n",
        "  # Generate the summary using the T5 model with beam search and length constraints\n",
        "  outs = model.generate(input_ids=input_ids,\n",
        "                        attention_mask=attention_mask,\n",
        "                        early_stopping=True,\n",
        "                        num_beams=3,\n",
        "                        num_return_sequences=1,\n",
        "                        no_repeat_ngram_size=2,\n",
        "                        min_length = 75,\n",
        "                        max_length=300)\n",
        "\n",
        "  # Decode the summary from the output ids and post-process it by capitalizing each sentence\n",
        "  dec = [tokenizer.decode(ids,skip_special_tokens=True) for ids in outs]\n",
        "  summary = dec[0]\n",
        "  summary = postprocesstext(summary)\n",
        "  summary = summary.strip()\n",
        "\n",
        "  # Return the summary\n",
        "  return summary\n",
        "\n",
        "# Call the summarizer function with the input text and generate the summary\n",
        "summarized_text = summarizer(text, summary_model, summary_tokenizer)\n",
        "\n",
        "# Function to split text into lines and indent them for better readability\n",
        "def pretty_print(text, width=80):\n",
        "    for line in text.split('\\n'):\n",
        "        print(line.ljust(width))\n",
        "\n",
        "# Print the original text\n",
        "print(\"\\nOriginal Text:\")\n",
        "pretty_print(text)\n",
        "\n",
        "# Print the summarized text\n",
        "print(\"\\nSummarized Text:\")\n",
        "pretty_print(summarized_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0YrWTxQCo9q"
      },
      "source": [
        "# **Keywords and Noun Phrases Span Extraction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "r9TmL5LR2Ucg"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet git+https://github.com/boudinfl/pke.git\n",
        "!pip install --quiet flashtext==2.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "84DxJGFn4MfD"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "\n",
        "# Download stopwords from the NLTK corpus\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Import the stopwords from the NLTK corpus\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Import the string module for punctuation characters\n",
        "import string\n",
        "\n",
        "# Import the pke package for keyphrase extraction\n",
        "import pke\n",
        "\n",
        "# Import the traceback module for debugging\n",
        "import traceback\n",
        "\n",
        "def get_nouns_multipartite(content):\n",
        "    # Initialize an empty list to store the extracted nouns\n",
        "    out = []\n",
        "\n",
        "    # Check if the content is empty\n",
        "    if not content:\n",
        "        # Print an error message and return the empty list\n",
        "        print(\"Error: content is empty\")\n",
        "        return out\n",
        "\n",
        "    # Try to execute the following code block\n",
        "    try:\n",
        "        # Initialize the MultipartiteRank extractor\n",
        "        extractor = pke.unsupervised.MultipartiteRank()\n",
        "\n",
        "        # Create a stoplist that includes punctuation characters,\n",
        "        # left and right brackets, and English stopwords\n",
        "        stoplist = list(string.punctuation)\n",
        "        stoplist += ['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-']\n",
        "        stoplist += stopwords.words('english')\n",
        "\n",
        "        # Load the content and stoplist into the extractor\n",
        "        extractor.load_document(input=content, stoplist=stoplist)\n",
        "\n",
        "        # Specify the part-of-speech (POS) tags of the nouns and proper nouns\n",
        "        pos = {'PROPN', 'NOUN'}\n",
        "\n",
        "        # Select the candidate phrases based on the POS tags\n",
        "        extractor.candidate_selection(pos=pos)\n",
        "\n",
        "        # Assign weights to the candidate phrases based on their relevance\n",
        "        extractor.candidate_weighting(alpha=1.1, threshold=0.75, method='average')\n",
        "\n",
        "        # Extract the top-ranked nouns as key phrases\n",
        "        keyphrases = extractor.get_n_best(n=15)\n",
        "\n",
        "        # Add the key phrases to the output list\n",
        "        for val in keyphrases:\n",
        "            out.append(val[0])\n",
        "\n",
        "    # If any exception occurs, print the error message and the traceback\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "    # Return the output list\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3dYIe8CZdf2C"
      },
      "outputs": [],
      "source": [
        "!pip install flashtext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "J_sRWHAd4Wwp"
      },
      "outputs": [],
      "source": [
        "from flashtext import KeywordProcessor\n",
        "\n",
        "def get_keywords(originaltext, summarytext):\n",
        "    # Extract keywords from the original text\n",
        "    keywords = get_nouns_multipartite(originaltext)\n",
        "    print(\"keywords unsummarized: \", keywords)\n",
        "\n",
        "    # Create a KeywordProcessor object to extract keywords from the summary text\n",
        "    keyword_processor = KeywordProcessor()\n",
        "\n",
        "    # Add the extracted keywords to the KeywordProcessor object\n",
        "    for keyword in keywords:\n",
        "        keyword_processor.add_keyword(keyword)\n",
        "\n",
        "    # Extract keywords from the summary text\n",
        "    keywords_found = keyword_processor.extract_keywords(summarytext)\n",
        "\n",
        "    # Convert the extracted keywords to a set and then back to a list to remove duplicates\n",
        "    keywords_found = list(set(keywords_found))\n",
        "    print(\"keywords_found in summarized: \", keywords_found)\n",
        "\n",
        "    # Identify the keywords that are present in both the original text and the summary text\n",
        "    important_keywords = []\n",
        "    for keyword in keywords:\n",
        "        if keyword in keywords_found:\n",
        "            important_keywords.append(keyword)\n",
        "\n",
        "    # Return the first four important keywords\n",
        "    return important_keywords[:4]\n",
        "\n",
        "\n",
        "imp_keywords = get_keywords(text, summarized_text)\n",
        "print(\"important keywords: \", imp_keywords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXbq7b2WCaZ_"
      },
      "source": [
        "# **Question generation with T5**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3CuKlpL1Cj6C"
      },
      "outputs": [],
      "source": [
        "question_model = T5ForConditionalGeneration.from_pretrained('ramsrigouthamg/t5_squad_v1')\n",
        "question_tokenizer = T5Tokenizer.from_pretrained('ramsrigouthamg/t5_squad_v1')\n",
        "question_model = question_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7uzA4uLJ_P48"
      },
      "outputs": [],
      "source": [
        "def get_question(context, answer, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Generates a question based on the given context and answer using a pre-trained model.\n",
        "\n",
        "    Args:\n",
        "        context (str): The context or passage from which the question is to be generated.\n",
        "        answer (str): The answer present in the context for which the question is to be generated.\n",
        "        model: The pre-trained model used for question generation.\n",
        "        tokenizer: The tokenizer used for encoding the text.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated question based on the context and answer.\n",
        "    \"\"\"\n",
        "    # Combine context and answer into a single text string\n",
        "    text = \"context: {} answer: {}\".format(context, answer)\n",
        "\n",
        "    # Tokenize and encode the text using the provided tokenizer\n",
        "    encoding = tokenizer.encode_plus(text, max_length=384, pad_to_max_length=False, truncation=True, return_tensors=\"pt\").to(device)\n",
        "    input_ids, attention_mask = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
        "\n",
        "    # Generate questions using the model\n",
        "    outs = model.generate(input_ids=input_ids,\n",
        "                          attention_mask=attention_mask,\n",
        "                          early_stopping=True,\n",
        "                          num_beams=5,\n",
        "                          num_return_sequences=1,\n",
        "                          no_repeat_ngram_size=2,\n",
        "                          max_length=72)\n",
        "\n",
        "    # Decode the generated question from token IDs and remove special tokens\n",
        "    dec = [tokenizer.decode(ids, skip_special_tokens=True) for ids in outs]\n",
        "    Question = dec[0].replace(\"question:\", \"\")\n",
        "    Question = Question.strip()\n",
        "    return Question\n",
        "\n",
        "\n",
        "# Print the summarized text, wrapping lines at 150 characters\n",
        "for wrp in wrap(summarized_text, 150):\n",
        "    print(wrp)\n",
        "print(\"\\n\")\n",
        "\n",
        "# Generate questions for important keywords and print them along with the keywords\n",
        "for answer in imp_keywords:\n",
        "    ques = get_question(summarized_text, answer, question_model, question_tokenizer)\n",
        "    print(ques)\n",
        "    print(answer.capitalize())\n",
        "    print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcV4uNlmApdW"
      },
      "source": [
        "# **Filter keywords with Maximal Marginal Relevance (MMR) algorithm to select a diverse set of keywords based on cosine similarity**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "asz04PIeQhHo"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet keybert==0.2.0\n",
        "!pip install --quiet strsim==0.0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FWi5WM67BLhL"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/explosion/sense2vec/releases/download/v1.0.0/s2v_reddit_2015_md.tar.gz\n",
        "!tar -xvf  s2v_reddit_2015_md.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PQJtZbsJ3wdk"
      },
      "outputs": [],
      "source": [
        "!pip install -q sense2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nVBkX2IoBgd8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sense2vec import Sense2Vec\n",
        "s2v = Sense2Vec().from_disk(\"s2v_old\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PAqN2OZbQ51Z"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "sentence_transformer_model = SentenceTransformer('msmarco-distilbert-base-v3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MCt8bDe0RSv-"
      },
      "outputs": [],
      "source": [
        "!pip install strsim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kv-FoyLEdCGE"
      },
      "outputs": [],
      "source": [
        "# Importing the Normalized Levenshtein distance calculation library\n",
        "from similarity.normalized_levenshtein import NormalizedLevenshtein\n",
        "\n",
        "# Initializing the Normalized Levenshtein object\n",
        "normalized_levenshtein = NormalizedLevenshtein()\n",
        "\n",
        "# Function to filter words based on their semantic sense\n",
        "def filter_same_sense_words(original, wordlist):\n",
        "    \"\"\"\n",
        "    Filters words from a given list that share the same semantic sense as the original word.\n",
        "    \n",
        "    Parameters:\n",
        "    - original: The original word whose sense we want to match against.\n",
        "    - wordlist: A list of words to filter through.\n",
        "    \n",
        "    Returns:\n",
        "    - A list of words from the input list that share the same sense as the original word.\n",
        "    \"\"\"\n",
        "    filtered_words = []  # List to store words of the same sense\n",
        "    base_sense = original.split('|')[1]  # Extracting the base sense from the original word\n",
        "    \n",
        "    # Iterating over each word in the wordlist\n",
        "    for eachword in wordlist:\n",
        "        # Checking if the sense of the current word matches the base sense\n",
        "        if eachword[0].split('|')[1] == base_sense:\n",
        "            # Adding the word to the filtered list after processing\n",
        "            filtered_words.append(eachword[0].split('|')[0].replace(\"_\", \" \").title().strip())\n",
        "    \n",
        "    return filtered_words\n",
        "\n",
        "# Function to calculate the highest similarity score between a word and a list of words\n",
        "def get_highest_similarity_score(wordlist, wrd):\n",
        "    \"\"\"\n",
        "    Calculates the highest similarity score between a given word and a list of words using the Normalized Levenshtein distance.\n",
        "    \n",
        "    Parameters:\n",
        "    - wordlist: A list of words to compare against.\n",
        "    - wrd: The word to find the highest similarity score for.\n",
        "    \n",
        "    Returns:\n",
        "    - The maximum similarity score found among the words in the list.\n",
        "    \"\"\"\n",
        "    score = []  # List to store similarity scores\n",
        "    \n",
        "    # Calculating similarity scores for each word in the list\n",
        "    for each in wordlist:\n",
        "        score.append(normalized_levenshtein.similarity(each.lower(), wrd.lower()))\n",
        "    \n",
        "    return max(score)\n",
        "\n",
        "# Function to retrieve words associated with a given sense using Sense2Vec\n",
        "def sense2vec_get_words(word, s2v, topn, question):\n",
        "    \"\"\"\n",
        "    Retrieves words associated with a given sense using Sense2Vec, filters them based on semantic similarity, and excludes words already present in the question.\n",
        "    \n",
        "    Parameters:\n",
        "    - word: The word to find similar words for.\n",
        "    - s2v: The Sense2Vec model instance.\n",
        "    - topn: The number of top similar words to retrieve.\n",
        "    - question: The question to exclude certain words from the result.\n",
        "    \n",
        "    Returns:\n",
        "    - A list of words that are semantically similar to the input word and not part of the question.\n",
        "    \"\"\"\n",
        "    output = []  # List to store the final set of words\n",
        "    \n",
        "    # Attempting to retrieve the best sense for the input word\n",
        "    try:\n",
        "        sense = s2v.get_best_sense(word, senses=[\"NOUN\", \"PERSON\", \"PRODUCT\", \"LOC\", \"ORG\", \"EVENT\", \"NORP\", \"WORK OF ART\", \"FAC\", \"GPE\", \"NUM\", \"FACILITY\"])\n",
        "        most_similar = s2v.most_similar(sense, n=topn)\n",
        "        \n",
        "        # Filtering words of the same sense from the most similar words\n",
        "        output = filter_same_sense_words(sense, most_similar)\n",
        "    except:\n",
        "        output = []\n",
        "    \n",
        "    threshold = 0.6  # Threshold for filtering words based on similarity score\n",
        "    final = [word]  # Initial list containing the input word\n",
        "    \n",
        "    # Excluding words already present in the question\n",
        "    checklist = question.split()\n",
        "    \n",
        "    # Filtering words based on similarity score and exclusion criteria\n",
        "    for x in output:\n",
        "        if get_highest_similarity_score(final, x) < threshold and x not in final and x not in checklist:\n",
        "            final.append(x)\n",
        "    \n",
        "    return final[1:]  # Returning the list excluding the initial word\n",
        "\n",
        "# Function to perform Maximal Marginal Relevance (MMR) keyword extraction\n",
        "def mmr(doc_embedding, word_embeddings, words, top_n, lambda_param):\n",
        "    \"\"\"\n",
        "    Performs Maximal Marginal Relevance (MMR) keyword extraction from a document embedding and a list of word embeddings.\n",
        "    \n",
        "    Parameters:\n",
        "    - doc_embedding: The document embedding vector.\n",
        "    - word_embeddings: The list of word embedding vectors.\n",
        "    - words: The list of words corresponding to the word embeddings.\n",
        "    - top_n: The number of top keywords to extract.\n",
        "    - lambda_param: The parameter balancing the trade-off between diversity and relevance in MMR.\n",
        "    \n",
        "    Returns:\n",
        "    - A list of top keywords extracted using MMR.\n",
        "    \"\"\"\n",
        "    word_doc_similarity = cosine_similarity(word_embeddings, doc_embedding)\n",
        "    word_similarity = cosine_similarity(word_embeddings)\n",
        "\n",
        "    keywords_idx = [np.argmax(word_doc_similarity)]  # Initial keyword index\n",
        "    candidates_idx = [i for i in range(len(words)) if i!= keywords_idx[0]]  # Candidate indices excluding the initial keyword\n",
        "\n",
        "    for _ in range(top_n - 1):  # Iterating until the desired number of keywords is reached\n",
        "        candidate_similarities = word_doc_similarity[candidates_idx, :]  # Similarity scores of candidates with the document\n",
        "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)  # Maximum similarity scores of candidates with the current keywords\n",
        "        \n",
        "        # Calculating MMR scores\n",
        "        mmr = (lambda_param) * candidate_similarities - (1-lambda_param) * target_similarities.reshape(-1, 1)\n",
        "        \n",
        "        # Finding the index of the candidate with the highest MMR score\n",
        "        mmr_idx = candidates_idx[np.argmax(mmr)]\n",
        "        \n",
        "        # Updating the list of keywords and removing the selected candidate from the candidates list\n",
        "        keywords_idx.append(mmr_idx)\n",
        "        candidates_idx.remove(mmr_idx)\n",
        "    \n",
        "    # Returning the list of keywords\n",
        "    return [words[idx] for idx in keywords_idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "w47gqAICs6VF"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3eigljekAu9i"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "from collections import OrderedDict\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Function to generate distractors using WordNet\n",
        "def get_distractors_wordnet(word):\n",
        "    \"\"\"\n",
        "    Generates distractors for a given word using WordNet, focusing on synonyms within the same sense.\n",
        "    \n",
        "    Parameters:\n",
        "    - word: The word for which distractors are to be generated.\n",
        "    \n",
        "    Returns:\n",
        "    - A list of distractors for the input word.\n",
        "    \"\"\"\n",
        "    distractors = []  # List to store distractors\n",
        "    \n",
        "    try:\n",
        "        # Retrieving the first noun synset for the word\n",
        "        syn = wn.synsets(word, 'n')[0]\n",
        "        \n",
        "        # Preparing the word for comparison\n",
        "        word = word.lower()\n",
        "        orig_word = word\n",
        "        if len(word.split()) > 0:\n",
        "            word = word.replace(\" \", \"_\")\n",
        "        \n",
        "        # Getting hypernyms of the synset\n",
        "        hypernym = syn.hypernyms()\n",
        "        \n",
        "        # If no hypernyms exist, returning an empty list\n",
        "        if len(hypernym) == 0:\n",
        "            return distractors\n",
        "        \n",
        "        # Iterating through hyponyms of the first hypernym\n",
        "        for item in hypernym[0].hyponyms():\n",
        "            name = item.lemmas()[0].name()\n",
        "            \n",
        "            # Skipping if the name is the same as the original word\n",
        "            if name == orig_word:\n",
        "                continue\n",
        "            \n",
        "            # Formatting the name\n",
        "            name = name.replace(\"_\", \" \")\n",
        "            name = \" \".join(w.capitalize() for w in name.split())\n",
        "            \n",
        "            # Adding unique names to the distractors list\n",
        "            if name is not None and name not in distractors:\n",
        "                distractors.append(name)\n",
        "    except:\n",
        "        print(\"Wordnet distractors not found\")\n",
        "    \n",
        "    return distractors\n",
        "\n",
        "# Function to generate distractors using Sense2Vec and Sentence Transformer models\n",
        "def get_distractors(word, origsentence, sense2vecmodel, sentencemodel, top_n, lambdaval):\n",
        "    \"\"\"\n",
        "    Generates distractors for a given word using both Sense2Vec and Sentence Transformer models, aiming to diversify the selection.\n",
        "    \n",
        "    Parameters:\n",
        "    - word: The word for which distractors are to be generated.\n",
        "    - origsentence: The original sentence containing the word.\n",
        "    - sense2vecmodel: The Sense2Vec model instance.\n",
        "    - sentencemodel: The Sentence Transformer model instance.\n",
        "    - top_n: The number of top similar words to consider.\n",
        "    - lambdaval: The lambda value for MMR keyword extraction.\n",
        "    \n",
        "    Returns:\n",
        "    - A list of diversified distractors for the input word.\n",
        "    \"\"\"\n",
        "    distractors = sense2vec_get_words(word, sense2vecmodel, top_n, origsentence)\n",
        "    print(\"distractors \", distractors)\n",
        "    \n",
        "    # If no distractors are found, returning an empty list\n",
        "    if len(distractors) == 0:\n",
        "        return distractors\n",
        "    \n",
        "    distractors_new = [word.capitalize()]  # Starting with the capitalized word itself\n",
        "    distractors_new.extend(distractors)  # Extending with the distractors found\n",
        "    \n",
        "    # Encoding the original sentence and the new distractors\n",
        "    embedding_sentence = origsentence + \" \" + word.capitalize()\n",
        "    keyword_embedding = sentencemodel.encode([embedding_sentence])\n",
        "    distractor_embeddings = sentencemodel.encode(distractors_new)\n",
        "    \n",
        "    # Determining the maximum number of keywords to keep\n",
        "    max_keywords = min(len(distractors_new), 5)\n",
        "    \n",
        "    # Applying MMR to filter keywords\n",
        "    filtered_keywords = mmr(keyword_embedding, distractor_embeddings, distractors_new, max_keywords, lambdaval)\n",
        "    \n",
        "    # Finalizing the list of keywords, excluding duplicates and the original word\n",
        "    final = [word.capitalize()]\n",
        "    for wrd in filtered_keywords:\n",
        "        if wrd.lower()!= word.lower():\n",
        "            final.append(wrd.capitalize())\n",
        "    final = final[1:]\n",
        "    \n",
        "    return final\n",
        "\n",
        "# Example usage\n",
        "sent = \"Which animal is called killer whale\"\n",
        "keyword = \"orca\"\n",
        "\n",
        "# Assuming s2v and sentence_transformer_model are defined elsewhere\n",
        "print(get_distractors(keyword, sent, s2v, sentence_transformer_model, 40, 0.2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogiuJdRgg7V6"
      },
      "source": [
        "# **JSON generation - MCQs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "AC0i2ECThAqW"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "# Function to generate a question along with its distractors and correct answers\n",
        "def generate_question(context, chooseOption):\n",
        "    \"\"\"\n",
        "    Generates a question based on a given context, including distractors and correct answers.\n",
        "    \n",
        "    Parameters:\n",
        "    - context: The text context from which to generate the question.\n",
        "    - chooseOption: Specifies whether to use WordNet or Sense2Vec for generating distractors.\n",
        "    \n",
        "    Returns:\n",
        "    - A tuple containing a list of questions and a summary of the context.\n",
        "    \"\"\"\n",
        "    # Summarize the context and extract keywords\n",
        "    summary_text = summarizer(context, summary_model, summary_tokenizer)\n",
        "    np = get_keywords(context, summary_text)\n",
        "\n",
        "    questions = []\n",
        "    for answer in np:\n",
        "        # Generate a question based on the answer\n",
        "        ques = get_question(summary_text, answer, question_model, question_tokenizer)\n",
        "        \n",
        "        # Determine distractors based on the chosen method\n",
        "        if chooseOption == \"Wordnet\":\n",
        "            distractors = get_distractors_wordnet(answer)\n",
        "        else:\n",
        "            distractors = get_distractors(answer.capitalize(), ques, s2v, sentence_transformer_model, 40, 0.2)\n",
        "\n",
        "        # Prepare correct and all answers\n",
        "        correct_answers = [answer.capitalize()]\n",
        "        all_answers = correct_answers + distractors[:4]\n",
        "        random.shuffle(all_answers)\n",
        "        answers = all_answers\n",
        "        correct_answer_indices = [i for i, answer in enumerate(answers) if answer in correct_answers]\n",
        "\n",
        "        # Construct the question dictionary\n",
        "        question = {\n",
        "            \"question\": ques,\n",
        "            \"answers\": answers,\n",
        "            \"correct_answer_indices\": correct_answer_indices,\n",
        "            \"correct_answers\": correct_answers\n",
        "        }\n",
        "        questions.append(question)\n",
        "\n",
        "    # Replace placeholders in the summary with actual answers\n",
        "    summary = f\"Summary: \\n{summary_text}\"\n",
        "    for answer in np:\n",
        "        summary = summary.replace(answer, f\"{answer}\")\n",
        "        summary = summary.replace(answer.capitalize(), f\"{answer.capitalize()}\")\n",
        "\n",
        "    return questions, summary\n",
        "\n",
        "# Context and radio button choice\n",
        "context = \"\"\"\n",
        "Your context goes here...\n",
        "\"\"\"\n",
        "chooseOption = \"Sense2Vec\"\n",
        "\n",
        "# Assuming other functions like summarizer, get_keywords, get_question, etc., are defined elsewhere\n",
        "import json\n",
        "questions, summary = generate_question(context, chooseOption)\n",
        "print(json.dumps(questions, indent=4))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "GfI2_yZVjWOk"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
